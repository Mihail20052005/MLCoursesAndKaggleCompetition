{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjjtruRfRtaW"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.linalg as sla\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHITb0dimYrx"
      },
      "source": [
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXuTAfcvk6kG"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def f(x):\n",
        "    \"\"\"\n",
        "    :param x: np.array(np.float) размерности 2\n",
        "    :return: float\n",
        "    \"\"\"\n",
        "    return np.sum(np.sin(x)**2, axis=0)\n",
        "\n",
        "\n",
        "def grad_f(x):\n",
        "    dx =  2*np.sin(x[0]) * np.cos(x[0])\n",
        "    dy = 2*np.sin(x[1]) * np.cos(x[1])\n",
        "    return np.array([dx,dy])\n",
        "\n",
        "\n",
        "def grad_descent_2d(f, grad_f, lr, num_iter=100, x0=None):\n",
        "    \"\"\"\n",
        "    функция, которая реализует градиентный спуск в минимум для функции f от двух переменных.\n",
        "        :param f: скалярная функция двух переменных\n",
        "        :param grad_f: функция, возвращающая градиент функции f (устроена как реализованная вами выше grad_f)\n",
        "        :param lr: learning rate алгоритма\n",
        "        :param num_iter: количество итераций градиентного спуска\n",
        "        :return: np.array[num_iter, 2] пар вида (x, f(x))\n",
        "    \"\"\"\n",
        "    if x0 is None:\n",
        "        x0 = np.random.random(2)\n",
        "\n",
        "    # будем сохранять значения аргументов и значений функции\n",
        "    # в процессе град. спуска в переменную history\n",
        "    history = []\n",
        "\n",
        "    # итерация цикла -- шаг градиентнго спуска\n",
        "    curr_x = x0.copy()\n",
        "    for iter_num in range(num_iter):\n",
        "        entry = np.hstack((curr_x, f(curr_x)))\n",
        "        history.append(entry)\n",
        "        # не забудьте про lr!\n",
        "        curr_x -= lr*grad_f(curr_x)\n",
        "\n",
        "    return np.vstack(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pejbp3fvMrWn"
      },
      "source": [
        "# Проверим, что градиент принимает вектор из двух чисел и выдает на этой точке верное значение\n",
        "assert np.allclose(grad_f(np.array([1, 2])),\n",
        "                   np.array([0.90929743, -0.7568025])), \"Что-то не так!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuR5OkOIGU9V"
      },
      "source": [
        "### Тестируем написанную функцию"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wb0cOqMfk_Rl"
      },
      "source": [
        "steps = grad_descent_2d(f, grad_f, lr=0.1, num_iter=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42db2E5qOyOP"
      },
      "source": [
        "steps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb_dn7QhlF6o"
      },
      "source": [
        "# %matplotlib osx\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import numpy as np\n",
        "\n",
        "path = []\n",
        "\n",
        "X, Y = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-3, 3, 100))\n",
        "\n",
        "fig = plt.figure(figsize=(16, 10))\n",
        "ax = fig.gca(projection='3d')\n",
        "\n",
        "zs = np.array([f(np.array([x,y]))\n",
        "              for x, y in zip(np.ravel(X), np.ravel(Y))])\n",
        "Z = zs.reshape(X.shape)\n",
        "\n",
        "\n",
        "ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, zorder=2)\n",
        "\n",
        "ax.plot(xs=steps[:, 0], ys=steps[:, 1], zs=steps[:, 2],\n",
        "        marker='*', markersize=20, zorder=3,\n",
        "        markerfacecolor='y', lw=3, c='black')\n",
        "\n",
        "ax.set_zlim(0, 5)\n",
        "ax.view_init(elev=60)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwf4pHzflIYz"
      },
      "source": [
        "Посмотрим на график значений функции от шага"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlO7-4hylVSU"
      },
      "source": [
        "plt.figure(figsize=(14,7))\n",
        "plt.xlabel('grad descent step number')\n",
        "plt.ylabel('$f(x)$')\n",
        "plt.title('Значение функции на каждом шаге гардиентного спуска.')\n",
        "\n",
        "f_values = list(map(lambda x: x[2], steps))\n",
        "plt.plot(f_values, label='gradient descent result')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bDnuMLgp-1e"
      },
      "source": [
        "def generate_batches(X, y, batch_size):\n",
        "    \"\"\"\n",
        "    param X: np.array[n_objects, n_features] --- матрица объекты-признаки\n",
        "    param y: np.array[n_objects] --- вектор целевых переменных\n",
        "    \"\"\"\n",
        "    assert len(X) == len(y)\n",
        "    np.random.seed(42)\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    perm = np.random.permutation(len(X))\n",
        "\n",
        "    for batch_start in <YOUR CODE>:\n",
        "        #YOUR CODE\n",
        "        yield #YOUR CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXYD6rpPsbIh"
      },
      "source": [
        "X_fake = np.arange(100)[:, np.newaxis]\n",
        "y_fake = np.arange(100) + 1000\n",
        "\n",
        "X_reconstructed, y_reconstructed = [], []\n",
        "for X_batch, y_batch in generate_batches(X_fake, y_fake, 10):\n",
        "    X_reconstructed.append(X_batch)\n",
        "    y_reconstructed.append(y_batch)\n",
        "\n",
        "X_reconstructed = np.concatenate(X_reconstructed)\n",
        "y_reconstructed = np.concatenate(y_reconstructed)\n",
        "\n",
        "assert (X_fake != X_reconstructed).all(), \"Что-то не так!\"\n",
        "assert (y_fake != y_reconstructed).all(), \"Что-то не так!\"\n",
        "\n",
        "assert (np.sort(X_reconstructed, axis=0) == X_fake).all(), \"Что-то не так!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acqZinh32YB6"
      },
      "source": [
        "Попробуем теперь *batch_size* не делящий размер датасета."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9AgUYVF2F6J"
      },
      "source": [
        "X_fake = np.arange(100)[:, np.newaxis]\n",
        "y_fake = np.arange(100) + 1000\n",
        "\n",
        "num_batches = 0\n",
        "for X_batch, y_batch in generate_batches(X_fake, y_fake, 7):\n",
        "    num_batches += 1\n",
        "\n",
        "assert num_batches == len(X_fake) // 7, \"Что-то не так!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5RcOhSbAidQ"
      },
      "source": [
        "def logit(x, w):\n",
        "    return np.dot(x, w)\n",
        "\n",
        "def sigmoid(h):\n",
        "    return 1. / (1 + np.exp(-h))\n",
        "\n",
        "\n",
        "class MyLogisticRegression(object):\n",
        "    def __init__(self):\n",
        "        self.w = None\n",
        "\n",
        "    def fit(self, X, y, epochs=10, lr=0.1, batch_size=100):\n",
        "        n, k = X.shape\n",
        "        if self.w is None:\n",
        "            np.random.seed(42)\n",
        "            # Вектор столбец в качестве весов\n",
        "            self.w = np.random.randn(k + 1)\n",
        "\n",
        "        X_train = np.concatenate((np.ones((n, 1)), X), axis=1)\n",
        "\n",
        "        losses = []\n",
        "\n",
        "        # Положите в лист losses лосс на каждом батче. Не нужно усреднять лосс по эпохе.\n",
        "\n",
        "        for i in range(epochs):\n",
        "            for X_batch, y_batch in generate_batches(X_train, y, batch_size):\n",
        "                #В X_train уже добавлен вектор 1\n",
        "\n",
        "                predictions = self._predict_proba_internal(X_batch)\n",
        "                loss = self.__loss(y_batch, predictions)#YOUR CODE: вычислите loss на текущем батче\n",
        "\n",
        "                assert (np.array(loss).shape == tuple()), \"Лосс должен быть скаляром!\"\n",
        "\n",
        "                losses.append(loss)\n",
        "                self.w -= lr * self.get_grad(X_batch, y_batch, predictions)\n",
        "                #YOUR CODE: обновите self.w по формуле градиентного спуска. Используйте функцию self.get_grad для вычисления градиента. Не забудьте про learning rate!\n",
        "\n",
        "        return losses\n",
        "\n",
        "    def get_grad(self, X_batch, y_batch, predictions):\n",
        "        \"\"\"\n",
        "\n",
        "        param X_batch: np.array[batch_size, n_features + 1] --- матрица объекты-признаки\n",
        "        param y_batch: np.array[batch_size] --- батч целевых переменных\n",
        "        param predictions: np.array[batch_size] --- батч вероятностей классов\n",
        "\n",
        "        Принимает на вход X_batch с уже добавленной колонкой единиц.\n",
        "        Выдаёт градиент функции потерь в логистической регрессии\n",
        "        как сумму градиентов функции потерь на всех объектах батча\n",
        "        ВНИМАНИЕ! Нулевая координата вектора весов -- это BIAS, а не вес признака.\n",
        "        Также не нужно ДЕЛИТЬ ГРАДИЕНТ НА РАЗМЕР БАТЧА:\n",
        "        нас интересует не среднее, а сумма.\n",
        "        В качестве оператора умножения матриц можно использовать @\n",
        "\n",
        "        Выход -- вектор-столбец градиентов для каждого веса (np.array[n_features + 1])\n",
        "        \"\"\"\n",
        "\n",
        "        #компонент градиента из логрегрессии\n",
        "        #следите за размерностями\n",
        "\n",
        "        grad_basic = X_batch.T @ (predictions - y_batch)\n",
        "        assert grad_basic.shape == (X_batch.shape[1],) , \"Градиенты должны быть столбцом из k_features + 1 элементов\"\n",
        "\n",
        "        return grad_basic\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        n, k = X.shape\n",
        "        X_ = np.concatenate((np.ones((n, 1)), X), axis=1)\n",
        "        return sigmoid(logit(X_, self.w))\n",
        "\n",
        "    def _predict_proba_internal(self, X):\n",
        "        \"\"\"\n",
        "        Возможно, вы захотите использовать эту функцию вместо predict_proba, поскольку\n",
        "        predict_proba конкатенирует вход с вектором из единиц, что не всегда удобно\n",
        "        для внутренней логики вашей программы\n",
        "        \"\"\"\n",
        "        return sigmoid(logit(X, self.w))\n",
        "\n",
        "    def predict(self, X, threshold=0.5):\n",
        "        return self.predict_proba(X) >= threshold\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.w.copy()\n",
        "        # copy тут используется неспроста. Если copy не использовать, то get_weights()\n",
        "        # выдаст ссылку на объект, а, значит, модифицируя результат применения функции\n",
        "        # get_weights(), вы модифицируете и веса self.w. Если вы хотите модифицировать веса,\n",
        "        # (например, в fit), используйте self.w\n",
        "\n",
        "    def __loss(self, y, p):\n",
        "        p = np.clip(p, 1e-10, 1 - 1e-10)\n",
        "        return -np.sum(y * np.log(p) + (1 - y) * np.log(1 - p))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOe-l11jClcZ"
      },
      "source": [
        "### Тестируем написанную функцию\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oex7f__9Cpsu"
      },
      "source": [
        "m = MyLogisticRegression()\n",
        "X = np.array([[1, 3, 4], [1, -5, 6], [-3, 5, 3]])\n",
        "X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
        "y = np.array([1, 0, 1])\n",
        "preds = np.array([.55, .22, .85])\n",
        "grads = m.get_grad(X, y, preds)\n",
        "assert np.allclose(grads, np.array([-0.38,  0.22, -3.2 , -0.93])), \"Что-то не так!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy7nGo7kR0bN"
      },
      "source": [
        "np.random.seed(42)\n",
        "m = MyLogisticRegression()\n",
        "X = np.random.rand(100,3)\n",
        "y = np.random.randint(0, 1, size=(100,))\n",
        "preds = np.random.rand(100)\n",
        "grads = m.get_grad(X, y, preds)\n",
        "assert np.allclose(grads, np.array([23.8698149, 25.27049356, 24.4139452])), \"Что-то не так!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZDT65fd7nCa"
      },
      "source": [
        "class MyElasticLogisticRegression(MyLogisticRegression):\n",
        "    def __init__(self, l1_coef, l2_coef):\n",
        "        self.l1_coef = l1_coef\n",
        "        self.l2_coef = l2_coef\n",
        "        self.w = None\n",
        "\n",
        "    def get_grad(self, X_batch, y_batch, predictions):\n",
        "        \"\"\"\n",
        "        Принимает на вход X_batch с уже добавленной колонкой единиц.\n",
        "        Выдаёт градиент функции потерь в логистической регрессии с регуляризаторами\n",
        "        как сумму градиентов функции потерь на всех объектах батча + регуляризационное слагаемое\n",
        "        ВНИМАНИЕ! Нулевая координата вектора весов -- это BIAS, а не вес признака.\n",
        "        Bias в регуляризационные слагаемые не входит. Также не нужно ДЕЛИТЬ ГРАДИЕНТ НА РАЗМЕР БАТЧА:\n",
        "        нас интересует не среднее, а сумма.\n",
        "\n",
        "        Выход -- вектор-столбец градиентов для каждого веса (np.array[n_features + 1])\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        grad_basic = X_batch.T @ (predictions - y_batch)\n",
        "\n",
        "        grad_l1 =  self.l1_coef * np.sign(self.w)#YOUR CODE: компонент градиента из l_1-регуляризации (не забудьте обнулить компоненту с bias)\n",
        "        grad_l2 = 2*self.l2_coef *self.w#YOUR CODE: компонент градиента из l_2-регуляризации (не забудьте обнулить компоненту с bias)\n",
        "\n",
        "        #Обнулять bias-компоненту вектора весов не нужно!\n",
        "        grad_l1[0] = 0\n",
        "        grad_l2[0] = 0\n",
        "        assert grad_l1[0] == grad_l2[0] == 0, \"Bias в регуляризационные слагаемые не входит!\"\n",
        "        assert grad_basic.shape == grad_l1.shape == grad_l2.shape == (X_batch.shape[1],) , \"Градиенты должны быть столбцом из k_features + 1 элементов\"\n",
        "\n",
        "        return grad_basic + grad_l1 + grad_l2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7U_nJDQehy9"
      },
      "source": [
        "### Тестирование\n",
        "Протестируем написанную функцию:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKow8JfgSgHz"
      },
      "source": [
        "m = MyElasticLogisticRegression(.2,.2)\n",
        "X = np.array([[1, 3, 4], [1, -5, 6], [-3, 5, 3]])\n",
        "X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
        "y = np.array([1, 0, 1])\n",
        "preds = np.array([.55, .22, .85])\n",
        "m.w = np.array([1,1,1,1])\n",
        "grads = m.get_grad(X, y, preds)\n",
        "assert np.allclose(grads, np.array([-0.38,  0.82, -2.6 , -0.33])), \"Что-то не так!\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GA-fvfcSuTg"
      },
      "source": [
        "np.random.seed(42)\n",
        "m = MyElasticLogisticRegression(.2, .2)\n",
        "X = np.random.rand(100,3)\n",
        "X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
        "y = np.random.randint(0, 1, size=(100,))\n",
        "preds = np.random.rand(100)\n",
        "m.w = np.array([1,1,1,1])\n",
        "grads = m.get_grad(X, y, preds)\n",
        "assert np.allclose(grads, np.array([49.11489408, 24.4698149, 25.87049356, 25.0139452])), \"Что-то не так!\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmLE5XsLSkH_"
      },
      "source": [
        "## Смотрим, как работает наша модель\n",
        "Протестируем на искусственных данных:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3_G7LzsehLx"
      },
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "X, y = make_blobs(n_samples=1000, centers=[[-2,0.5],[3,-0.5]], cluster_std=1, random_state=42)\n",
        "#y = y.reshape(-1, 1)\n",
        "\n",
        "colors = (\"red\", \"green\")\n",
        "colored_y = np.zeros(y.size, dtype=str)\n",
        "\n",
        "for i, cl in enumerate([0,1]):\n",
        "    colored_y[y.ravel() == cl] = str(colors[i])\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=colored_y)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e83rZRceqn-"
      },
      "source": [
        "clf = MyElasticLogisticRegression(0.1, 0.1)\n",
        "clf.fit(X, y, epochs=1000)\n",
        "w = clf.get_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9LnB1Xffkql"
      },
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "\n",
        "eps = 0.1\n",
        "xx, yy = np.meshgrid(np.linspace(np.min(X[:,0]) - eps, np.max(X[:,0]) + eps, 200),\n",
        "                     np.linspace(np.min(X[:,1]) - eps, np.max(X[:,1]) + eps, 200))\n",
        "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA'])\n",
        "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=colored_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBaJqkC9g8ID"
      },
      "source": [
        "Так как мы сделали классификатор для двух классов, то мы выберем из всех картинок только картинки 0 и 1, првратим их из двумерной матрицы в вектор и обучим нашу модель."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_DfAkIvg6GF"
      },
      "source": [
        "data = pd.read_csv('./train.csv')\n",
        "#файл лежит в директории с домашним заданием. Чтобы иметь возможность его\n",
        "#открыть, загрузите его на колаб (панель слева, нажимаем значок \"Файлы\",\n",
        "#выбираем \"загрузить в сессионное хранилище\", выбираем нужный файл)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zDXXmEmw3dW"
      },
      "source": [
        "Данные выглядят как таблица, первая колонка которой это лейбел -- цифра, которая изображена на картинке, а следующие 784 колонки это значения от 0 до 255, описывающие все пиксели картинки 28х28. То есть нам не придется делать reshape и превращать матрицу в вектор, так как это уже сделано."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be-k7gu-w3dW"
      },
      "source": [
        "X = data.iloc[:, 1:]\n",
        "y = data.iloc[:, 0]\n",
        "\n",
        "# Выберем только картинки, где изображен 0 и 1\n",
        "X = X[(y == 0) | (y == 1)]\n",
        "y = y[(y == 0) | (y == 1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrBJh4stw3dZ"
      },
      "source": [
        "Для оценки модели мы используем кросс валидацию."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vaS7Y3kw3da"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# YOUR CODE\n",
        "# Сделайте sklearn pipeline с StandarScaler и MyElasticLogisticRegression и проверьте точность с помощью cross_val_score.\n",
        "# в качестве scorer используйте 'accuracy'. Эта часть не проверяется в степике.\n",
        "\n",
        "\n",
        "print(f\"Mean accuracy of Logistic Regression for two classes is {mean_accuracy}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH7avq9xM9O0"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}